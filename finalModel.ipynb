{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment                                               Text\n",
      "0  positive  like summerslam look arena curtain look overal...\n",
      "1  positive  television show appeal different kind fan like...\n",
      "2  negative  film quickly get major chase scene increase de...\n",
      "3  positive  jane austen definitely approve onebr br gwynet...\n",
      "4  negative  expectation somewhat high go movie think steve...\n",
      "Sentiment\n",
      "positive    20004\n",
      "negative    19996\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Model Comparison:\n",
      "              Model  Accuracy  Precision  Recall  F1-Score\n",
      "                SVM    0.8904     0.8907  0.8904    0.8904\n",
      "Logistic Regression    0.8894     0.8899  0.8894    0.8893\n",
      "        Naive Bayes    0.8696     0.8702  0.8696    0.8696\n",
      "                ANN    0.8686     0.8687  0.8686    0.8686\n",
      "      Random Forest    0.8585     0.8587  0.8585    0.8585\n",
      "                KNN    0.7596     0.7648  0.7596    0.7584\n",
      "\n",
      "Best Model: SVM\n",
      "Best Model Metrics:\n",
      "Accuracy: 0.8904\n",
      "Precision: 0.8907\n",
      "Recall: 0.8904\n",
      "F1-Score: 0.8904\n",
      "\n",
      "Confusion Matrices:\n",
      "\n",
      "Confusion Matrix for SVM:\n",
      "True\\Pred\t0\t1\t\n",
      "----------------------------------------\n",
      "0\t3505\t494\t\n",
      "1\t383\t3618\t\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "True\\Pred\t0\t1\t\n",
      "----------------------------------------\n",
      "0\t3390\t609\t\n",
      "1\t523\t3478\t\n",
      "\n",
      "Confusion Matrix for ANN:\n",
      "True\\Pred\t0\t1\t\n",
      "----------------------------------------\n",
      "0\t3454\t545\t\n",
      "1\t506\t3495\t\n",
      "\n",
      "Confusion Matrix for Naive Bayes:\n",
      "True\\Pred\t0\t1\t\n",
      "----------------------------------------\n",
      "0\t3401\t598\t\n",
      "1\t445\t3556\t\n",
      "\n",
      "Confusion Matrix for KNN:\n",
      "True\\Pred\t0\t1\t\n",
      "----------------------------------------\n",
      "0\t2759\t1240\t\n",
      "1\t683\t3318\t\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      "True\\Pred\t0\t1\t\n",
      "----------------------------------------\n",
      "0\t3484\t515\t\n",
      "1\t370\t3631\t\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from joblib import Parallel, delayed\n",
    "import spacy\n",
    "from contractions import fix\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Enhanced preprocessing function\n",
    "def clean_text(text):\n",
    "    text = fix(text)  # Expand contractions\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization and lemmatization with SpaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='ISO-8859-1')\n",
    "    df['Text'] = df['Text'].apply(clean_text)\n",
    "    print(df.head())\n",
    "    print(df['Sentiment'].value_counts())  # Check class distribution\n",
    "    return df\n",
    "\n",
    "# Data splitting\n",
    "def split_data(df):\n",
    "    X = df['Text']\n",
    "    y = df['Sentiment']\n",
    "\n",
    "    # Vectorize text\n",
    "    vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 3), min_df=5, max_df=0.8)\n",
    "    X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "# Cross-validation with hyperparameter tuning and parallel processing\n",
    "def cross_validate_model(model, X, y, param_grid=None):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    best_model = None\n",
    "    best_score = -1\n",
    "\n",
    "    def train_fold(train_idx, val_idx):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        if param_grid:\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1_weighted', cv=3, n_jobs=-1, verbose=1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            return grid_search.best_estimator_, grid_search.best_score_\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            score = model.score(X_val, y_val)\n",
    "            return model, score\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(train_fold)(train_idx, val_idx) for train_idx, val_idx in skf.split(X, y))\n",
    "\n",
    "    for trained_model, score in results:\n",
    "        if score > best_score:\n",
    "            best_model = trained_model\n",
    "            best_score = score\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate(models, X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "\n",
    "    for name, model_info in models.items():\n",
    "        model, param_grid = model_info\n",
    "        try:\n",
    "            model = cross_validate_model(model, X_train, y_train, param_grid)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "            acc = accuracy_score(y_test, predictions)\n",
    "            report = classification_report(y_test, predictions, zero_division=1, output_dict=True)\n",
    "            cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "            results[name] = {\n",
    "                'accuracy': acc,\n",
    "                'classification_report': report,\n",
    "                'confusion_matrix': cm,\n",
    "                'model': model\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[name] = {'error': str(e)}\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Compare models\n",
    "def compare_models(results):\n",
    "    metrics = []\n",
    "\n",
    "    for model_name, result in results.items():\n",
    "        if 'classification_report' not in result:\n",
    "            print(f\"Skipping {model_name} due to missing classification report.\")\n",
    "            continue\n",
    "\n",
    "        report = result['classification_report']\n",
    "        metrics.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Precision': report['weighted avg']['precision'],\n",
    "            'Recall': report['weighted avg']['recall'],\n",
    "            'F1-Score': report['weighted avg']['f1-score']\n",
    "        })\n",
    "\n",
    "    if not metrics:\n",
    "        raise ValueError(\"No valid model metrics available for comparison.\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics).round(4).sort_values('F1-Score', ascending=False)\n",
    "    best_model_name = metrics_df.iloc[0]['Model']\n",
    "    best_model_metrics = metrics_df.iloc[0].to_dict()\n",
    "\n",
    "    return metrics_df, best_model_name, best_model_metrics\n",
    "\n",
    "# Print confusion matrices\n",
    "def print_confusion_matrices(results):\n",
    "    for model_name, result in results.items():\n",
    "        if 'confusion_matrix' not in result:\n",
    "            continue\n",
    "\n",
    "        cm = result['confusion_matrix']\n",
    "        print(f\"\\nConfusion Matrix for {model_name}:\")\n",
    "        print(\"True\\\\Pred\\t\", end=\"\")\n",
    "        for i in range(cm.shape[1]):\n",
    "            print(f\"{i}\\t\", end=\"\")\n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "        for i in range(cm.shape[0]):\n",
    "            print(f\"{i}\\t\", end=\"\")\n",
    "            for j in range(cm.shape[1]):\n",
    "                print(f\"{cm[i,j]}\\t\", end=\"\")\n",
    "            print()\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = 'reduced_dataset.csv'\n",
    "    df = load_and_preprocess_data(filepath)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, vectorizer = split_data(df)\n",
    "\n",
    "    models = {\n",
    "        'SVM': (SVC(kernel='linear', probability=True), {'C': [0.1, 1, 10]}),\n",
    "        'Random Forest': (RandomForestClassifier(n_jobs=-1), {'n_estimators': [100, 200]}),\n",
    "        'ANN': (MLPClassifier(max_iter=1000), {'hidden_layer_sizes': [(128,), (128, 64)], 'alpha': [0.0001, 0.001], 'learning_rate': ['constant', 'adaptive']}),\n",
    "        'Naive Bayes': (MultinomialNB(), {'alpha': [0.1, 0.5, 1]}),\n",
    "        'KNN': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "        'Logistic Regression': (LogisticRegression(max_iter=1000, n_jobs=-1), {'C': [0.01, 0.1, 1]})\n",
    "    }\n",
    "\n",
    "    results = train_and_evaluate(models, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    metrics_df, best_model_name, best_model_metrics = compare_models(results)\n",
    "\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "\n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    print(\"Best Model Metrics:\")\n",
    "    for metric, value in best_model_metrics.items():\n",
    "        if metric != 'Model':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    print_confusion_matrices(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'model_results_40000.txt'\n"
     ]
    }
   ],
   "source": [
    "# Save metrics, confusion matrices, hyperparameters, and best model details to a file\n",
    "output_file = 'model_results_40000.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # Save model comparison metrics\n",
    "    f.write(\"Model Comparison Metrics:\\n\")\n",
    "    f.write(metrics_df.to_string(index=False))\n",
    "    \n",
    "    # Save best model details\n",
    "    f.write(\"\\n\\nBest Model:\\n\")\n",
    "    f.write(f\"{best_model_name}\\n\")\n",
    "    f.write(\"Best Model Metrics:\\n\")\n",
    "    for metric, value in best_model_metrics.items():\n",
    "        if metric != 'Model':\n",
    "            f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "    \n",
    "    # Save hyperparameters for all models\n",
    "    f.write(\"\\n\\nHyperparameters for All Models:\\n\")\n",
    "    for model_name, result in results.items():\n",
    "        model = result['model']\n",
    "        f.write(f\"\\n{model_name} Hyperparameters:\\n\")\n",
    "        if hasattr(model, 'get_params'):  # Check if hyperparameters are available\n",
    "            params = model.get_params()\n",
    "            for param, value in params.items():\n",
    "                f.write(f\"{param}: {value}\\n\")\n",
    "        else:\n",
    "            f.write(\"Hyperparameters not available for this model.\\n\")\n",
    "    \n",
    "    # Save confusion matrices\n",
    "    f.write(\"\\n\\nConfusion Matrices:\\n\")\n",
    "    for model_name, result in results.items():\n",
    "        f.write(f\"\\nConfusion Matrix for {model_name}:\\n\")\n",
    "        cm = result['confusion_matrix']\n",
    "        f.write(\"True\\\\Pred\\t\" + \"\\t\".join(map(str, range(cm.shape[1]))) + \"\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        for i in range(cm.shape[0]):\n",
    "            f.write(f\"{i}\\t\" + \"\\t\".join(map(str, cm[i])) + \"\\n\")\n",
    "\n",
    "print(f\"Results saved to '{output_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model and vectorizer saved as 'best_model.joblib' and 'vectorizer.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# Save the best model and vectorizer\n",
    "best_model_name = metrics_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']  # Retrieve the best model\n",
    "joblib.dump(best_model, 'best_model_1.joblib')  # Save the best model\n",
    "joblib.dump(vectorizer, 'vectorizer_1.joblib')  # Save the vectorizer\n",
    "print(f\"Best model and vectorizer saved as 'best_model.joblib' and 'vectorizer.joblib'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the best model and vectorizer\n",
    "best_model = joblib.load('best_model_1.joblib')  \n",
    "vectorizer = joblib.load('vectorizer_1.joblib')  \n",
    "\n",
    "\n",
    "def predict_sentiment(user_input):\n",
    "    # Preprocess the input text \n",
    "    clean_input = clean_text(user_input)\n",
    "    \n",
    "    # Transform the input using the saved vectorizer\n",
    "    input_vec = vectorizer.transform([clean_input])\n",
    "    \n",
    "    # Predict sentiment using the loaded model\n",
    "    prediction = best_model.predict(input_vec)\n",
    "    \n",
    "    # Return the predicted sentiment\n",
    "    return prediction[0]\n",
    "\n",
    "# User Input\n",
    "user_input = input(\"Enter text to predict sentiment: \")\n",
    "predicted_sentiment = predict_sentiment(user_input)\n",
    "\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
